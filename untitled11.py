# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wk-6BA7YmaVCEH0Ef1cMW3u-rOoPl6ZN
"""

!pip3 install stanza

!python3 sentence_scramble.py

ls

cd drive/

ls

cd MyDrive/SueNes/pre

ls

!python3 sentence_scramble.py

#dont run, dummy
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torch.utils.data import Dataset
# from torch.utils.data import DataLoader

# class CustomDataset(Dataset):
#     def __init__(self, datapath='/content/drive/MyDrive/SueNes/exp/data/cnn_dailymail/sent_delete_sent/test.tsv', nums=None, hierarchical=False):
#         self.data = []
#         print("Hierarchichal", hierarchical)
#         with open(datapath, "r", encoding="utf-8") as f:
#             for line in f:
#                 elements = line.split('\t')
#                 size = len(elements)
#                 for i in range(1, size-1):
#                     if hierarchical:
#                         self.data.append([elements[0], elements[i], elements[i+1]])
#                     else:
#                         self.data.append([elements[0], elements[1], elements[2]]) #changed the value from i+1 to 2
#                 # Limit the number of lines used
#                 if nums is not None:
#                     nums -= 1
#                     if nums == 0:
#                         break


#     def __len__(self):
#         return len(self.data)

#     def __getitem__(self, idx):
#         return self.data[idx][0], self.data[idx][1], self.data[idx][2]

pwd

#Data Loader code
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

class CustomDataset(Dataset):
    def __init__(self, datapath='/content/drive/MyDrive/SueNes/exp/data/cnn_dailymail/sent_delete_sent/train.tsv', nums=None, hierarchical=False):
        self.data = []
        print("Hierarchichal", hierarchical)
        with open(datapath, "r", encoding="utf-8") as f:
            for line in f:
                elements = line.split('\t')
                size = len(elements)
                for i in range(0, size-1, 3):
                    if hierarchical:
                        self.data.append([elements[0], elements[i], elements[i+1]])
                    else:
                        self.data.append([elements[i], elements[1], elements[i+2]]) #changed the value from i+1 to 2
                        # i = i + 2
                # Limit the number of lines used
                if nums is not None:
                    nums -= 1
                    if nums == 0:
                        break


    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx][0], self.data[idx][1], self.data[idx][2]

!pip3 install config

cd drive/MyDrive/SueNes/exp/data/cnn_dailymail/sent_delete_sent/

pwd

import config as CFG
train_set = CustomDataset('train.tsv')
train = train_set
print(len(train_set))

print(train_set[0][1])

from datasets import load_dataset
# dataset = load_dataset('tsv', data_files=['train.tsv', 'test.tsv'])
dataset = load_dataset('tsv', data_files='train.tsv')

!pip3 install transformers datasets
import transformers
import torch
# from datasets import load_dataset

# dataset = load_dataset("train.tsv")

# dataset["train"][100]

from transformers import AutoModel

model = AutoModel.from_pretrained("google/bert_uncased_L-2_H-128_A-2")

!pip3 install torch==1.12.1

!pip3 install torchdata

!pip3 install --upgrade transformers
!pip3 install --upgrade datasets

ls

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google/bert_uncased_L-2_H-128_A-2")
from torchdata.datapipes.map import SequenceWrapper, Mapper

tokenized_datasets = []

def tokenize_function(examples):
  return tokenizer(examples, padding="max_length", truncation=True)

# xy = 'Burns will be thrust into the No 10 shirt in enforced absence of Owen Farrell and Stephen Myler.'
# print(xy[0])
# xyz = tokenize_function(xy)
# inputs = tokenizer(train_set[0])
# inputs = tokenizer(

#     'HuggingFace is a company based in Paris and New York'

# )

# inputs = tokenizer(

#     'HuggingFace is a company based in Paris and New York', add_special_tokens=False, return_tensors="pt"

# )
# print(inputs)

for i in range()
  # tokenized_datasets[i] = tokenize_function(train_testing[i])
  cvs = tokenize_function(train_set[i])
  tokenized_datasets.append(cvs)

# tokenized_datasets = train_set.map(tokenize_function, batched=True)
# print(xyz)
print(tokenized_datasets)

print(train_set[0])

"""# TEST - IGNORE"""

# import os
# from transformers import BertTokenizer, BertModel, BertTokenizerFast, logging
# from tqdm import tqdm
# import argparse
# import config as CFG

# if __name__ == "__main__":
#   parser = argparse.ArgumentParser(
#         description="Train the model from a preprocessed dataset.")
#   parser.add_argument('--dataset', '-d', default='cnn_dailymail',
#                         help="Support 'cnn_dailymail', 'big_patent' or 'scientific_papers'.")
  
#   args = parser.parse_args()
  
#   DATASET = args.dataset
  
#   train_set = CustomDataset(os.path.join(CFG.DATASET_ROOT, DATASET, CFG.METHOD, 'train.tsv'))
  
#   print(len(train_set))

!pip3 install transformers datasets

import transformers

ls

# import torch
# from datasets import load_dataset

# dataset = '/content/drive/MyDrive/SueNes/exp/data/cnn_dailymail/sent_delete_sent/train.tsv'

# # dataset = torch.load('/content/drive/MyDrive/SueNes/exp/data/cnn_dailymail/sent_delete_sent/train.tsv')

# dataset[1]

cd /content/drive/MyDrive/SueNes/exp/data/cnn_dailymail/sent_delete_sent/

ls

# from datasets import load_dataset

# dataset = ["train.tsv"]

from transformers import AutoModel

model = AutoModel.from_pretrained("google/bert_uncased_L-2_H-128_A-2")


# Download pytorch model
model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)


# Transform input tokens 
inputs = tokenizer(dataset, return_tensors="pt")

# Model apply
outputs = model(**inputs)

from transformers import pipeline

classifier = pipeline(task = 'summarization', model = model_name, tokenizer = tokenizer)

classifier

print(inputs)
print(outputs)

encoded_input = tokenizer("Do not meddle in the affairs of wizards.")
print(encoded_input)

#important final code for data preprocessing
from transformers import AutoTokenizer
dataset = '/content/drive/MyDrive/SueNes/exp/data/cnn_dailymail/sent_delete_sent/train.tsv'
checkpoint = "prajjwal1/bert-tiny"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
def tokenize_function(examples):
    return tokenizer(examples["article"], padding="max_length", truncation=True)
tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets
#We use padding max length while taking the batched as True because it greatly saves time
#Our tokenize_function returns a dictionary with the keys input_ids, attention_mask, and token_type_ids
# from transformers import DataCollatorWithPadding

# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

